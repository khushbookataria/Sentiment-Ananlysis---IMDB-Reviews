{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis is a an NLP task that deals with analyzing a piece of text (eg: sentence, comment, document)  and returning a prediction wheteher the text conveys a positive or a negative sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Description:\n",
    "Data Set: Large Movie Review Dataset commonly known as the IMDB dataset with equal number of observstions in the training and testing set (25k reviews in each set). Also, both data sets contain equal number of positive and negative reviews. The data was collected by Stanford researchers and was used in the paper 'Learning Word Vectors for Sentiment Analysis'\n",
    "\n",
    "The problem is to determine whether a given movie review is a positive or a negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the important libraries\n",
    "\n",
    "# To suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Libraries for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Libraries for vectorizing input data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Libraries for building a model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Libraries for assessing the quality of model prediction\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the root directory\n",
    "root_dir = 'E:/Sentiment_Analysis/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's now read the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(root_dir + 'train.csv')\n",
    "df_test = pd.read_csv(root_dir + 'test.csv')\n",
    "combine = [df_train, df_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_9.txt</td>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000_8.txt</td>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001_10.txt</td>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                             review  sentiment\n",
       "0       0_9.txt  Bromwell High is a cartoon comedy. It ran at t...          1\n",
       "1   10000_8.txt  Homelessness (or Houselessness as George Carli...          1\n",
       "2  10001_10.txt  Brilliant over-acting by Lesley Ann Warren. Be...          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    12500\n",
      "0    12500\n",
      "Name: sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df_train[df_train.sentiment == 1].sentiment,\n",
    "         bins=2, color='green', label='Positive')\n",
    "plt.hist(df_train[df_train.sentiment == 0].sentiment,\n",
    "         bins=2, color='blue', label='Negative')\n",
    "plt.title('Sentiment distribution in the train set', fontsize=20)\n",
    "plt.xticks([])\n",
    "plt.xlim(-0.5, 2.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.clf()\n",
    "print(df_train.sentiment.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25000</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>25000</td>\n",
       "      <td>24904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>12216_4.txt</td>\n",
       "      <td>How has this piece of crap stayed on TV this l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                             review\n",
       "count         25000                                              25000\n",
       "unique        25000                                              24904\n",
       "top     12216_4.txt  How has this piece of crap stayed on TV this l...\n",
       "freq              1                                                  3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### we do not need the id column in the training dataset so we can drop that and take only the uniqu observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before (25000, 3) (25000, 3) (25000, 3) (25000, 3)\n",
      "After (25000, 2) (25000, 3) (25000, 2) (25000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Before\", df_train.shape, df_test.shape, combine[0].shape, combine[1].shape)\n",
    "\n",
    "df_train = df_train.drop('id', axis=1)\n",
    "combine = [df_train, df_test]\n",
    "\n",
    "print(\"After\", df_train.shape, df_test.shape, combine[0].shape, combine[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24904, 2)\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.drop_duplicates()\n",
    "print(df_train.shape)\n",
    "combine = [df_train, df_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning and pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.review[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean up the raw text, I'll use regular expressions (regex or regexp). It's a sequence of characters that forms a search pattern. The package re can be used to work with regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "def preprocessReviews(data):\n",
    "    data.review = [REPLACE_NO_SPACE.sub(\"\", text.lower()) for text in data.review]\n",
    "    data.review = [REPLACE_WITH_SPACE.sub(\" \", text) for text in data.review]\n",
    "    \n",
    "    return data\n",
    "\n",
    "df_train_clean = preprocessReviews(df_train)\n",
    "df_test_clean = preprocessReviews(df_test)\n",
    "\n",
    "combine = [df_train_clean, df_test_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell highs satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled  at  high a classic line inspector im here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isnt\n"
     ]
    }
   ],
   "source": [
    "print(df_train_clean.review[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i went and saw this movie last night after being coaxed to by a few friends of mine ill admit that i was reluctant to see it because from what i knew of ashton kutcher he was only able to do comedy i was wrong kutcher played the character of jake fischer very well and kevin costner played ben randall with such professionalism the sign of a good movie is that it can toy with our emotions this one did exactly that the entire theater which was sold out was overcome by laughter during the first half of the movie and were moved to tears during the second half while exiting the theater i not only saw many women in tears but many full grown men as well trying desperately not to let anyone see them crying this movie was great and i suggest that you go see it before you judge\n"
     ]
    }
   ],
   "source": [
    "print(df_test_clean.review[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In order to build the model, we need to convert each review to a numerical representation which is callled vectorization.\n",
    "This can be done by creating one large matrix with one column for every unique word in the corpus (where the corpus is all 50k reviews here). Then we transform each review into one row containing 0s and 1s, where 1 means that the word in the corpus corresponding to that column appears in that review. This process is also known as one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(df_train_clean.review)\n",
    "X = cv.transform(df_train_clean.review)\n",
    "X_test = cv.transform(df_test_clean.review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "I'll start with Logistic Regression as it serves as a good baseline model as -\n",
    "1) It's easy to interpret\n",
    "2) Linear models tend to perform better on sparsed data sets\n",
    "3) They learn very fast compared to otehr algorithms\n",
    "\n",
    "For the simplicity of the model, I'm only going to tune the hyperparameter c which adjusts the regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.8718278188242853\n",
      "Accuracy for C=0.05: 0.8809829746225506\n",
      "Accuracy for C=0.25: 0.8788949566334725\n",
      "Accuracy for C=0.5: 0.876164471570832\n",
      "Accuracy for C=1: 0.8737552200449727\n"
     ]
    }
   ],
   "source": [
    "#label = [1 if i < 12500 else 0 for i in range(25000)]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, df_train.sentiment, train_size = 0.75\n",
    ")\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:    ##How do we choose these values\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))       ## what's % for? What's %s?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c = 0.05 gives the best accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the model\n",
    "Now, with the optimal value of c, we can train our model on the entire training set and evaluate the accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.88196\n"
     ]
    }
   ],
   "source": [
    "final_model = LogisticRegression(C=0.05)\n",
    "final_model.fit(X, df_train.sentiment)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(df_test.sentiment, final_model.predict(X_test)))        ## Explain the syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the 5 most positive and negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('excellent', 0.927752572440215)\n",
      "('perfect', 0.7961497564740038)\n",
      "('great', 0.6739653530317321)\n",
      "('amazing', 0.6109258198225315)\n",
      "('superb', 0.6072850429678195)\n"
     ]
    }
   ],
   "source": [
    "feature_to_coef = {\n",
    "    word: coef for word, coef in zip(\n",
    "        cv.get_feature_names(), final_model.coef_[0]\n",
    "    )\n",
    "}\n",
    "for best_positive in sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1], \n",
    "    reverse=True)[:5]:\n",
    "    print (best_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('worst', -1.3590213521105312)\n",
      "('waste', -1.166796891505745)\n",
      "('awful', -1.0208874507011403)\n",
      "('poorly', -0.8749792787852262)\n",
      "('boring', -0.8578772745807502)\n"
     ]
    }
   ],
   "source": [
    "for best_negative in sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1])[:5]:\n",
    "    print (best_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Cleaning the Data - Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high cartoon comedy ran time programs school life teachers 35 years teaching profession lead believe bromwell highs satire much closer reality teachers scramble survive financially insightful students see right pathetic teachers pomp pettiness whole situation remind schools knew students saw episode student repeatedly tried burn school immediately recalled high classic line inspector im sack one teachers student welcome bromwell high expect many adults age think bromwell high far fetched pity isnt'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_stop_words = stopwords.words('english')\n",
    "\n",
    "def removeStopWords(review_list):\n",
    "    without_stop_words = []\n",
    "    for review in review_list:\n",
    "        without_stop_words.append(\n",
    "            ' '.join([word for word in review.split() \n",
    "                      if word not in english_stop_words])\n",
    "        )\n",
    "    return without_stop_words\n",
    "\n",
    "no_stop_words_train = removeStopWords(df_train_clean.review)\n",
    "no_stop_words_test = removeStopWords(df_test_clean.review)\n",
    "no_stop_words_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.8798586572438163\n",
      "Accuracy for C=0.05: 0.8882107292001284\n",
      "Accuracy for C=0.25: 0.8835528429168005\n",
      "Accuracy for C=0.5: 0.8803405075489881\n",
      "Accuracy for C=1: 0.8764857051076133\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(no_stop_words_train)\n",
    "X = cv.transform(no_stop_words_train)\n",
    "X_test = cv.transform(no_stop_words_test)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, df_train.sentiment, train_size = 0.75\n",
    ")\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.864760681015098\n",
      "Accuracy for C=0.05: 0.875361387728879\n",
      "Accuracy for C=0.25: 0.8732733697398009\n",
      "Accuracy for C=0.5: 0.8721490523610665\n",
      "Accuracy for C=1: 0.8699004176035978\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(binary=True, stop_words = 'english')\n",
    "cv.fit(no_stop_words_train)\n",
    "X = cv.transform(no_stop_words_train)\n",
    "X_test = cv.transform(no_stop_words_test)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, df_train.sentiment, train_size = 0.75\n",
    ")\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Normalization - Stemming and Lemmatization\n",
    "Text Normaliztion is the process of converting different words of same meaning into a single word. For example: sleep, sleeping, slept, asleep have the same root 'sleep'. Machine would treat these words differently. So in order to have the machine treat them the same we do text normalization. It can achieved using two processes viz. Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.8766463218760039\n",
      "Accuracy for C=0.05: 0.8854802441374879\n",
      "Accuracy for C=0.25: 0.8817860584645036\n",
      "Accuracy for C=0.5: 0.879537423707035\n",
      "Accuracy for C=1: 0.8772887889495663\n",
      "Final Accuracy: 0.87764\n"
     ]
    }
   ],
   "source": [
    "def getStemmedText(review_list):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [' '.join([stemmer.stem(word) for word in review.split()]) for review in review_list]\n",
    "\n",
    "stemmed_reviews_train = getStemmedText(df_train_clean.review)\n",
    "stemmed_reviews_test = getStemmedText(df_test_clean.review)\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(stemmed_reviews_train)\n",
    "X = cv.transform(stemmed_reviews_train)\n",
    "X_test = cv.transform(stemmed_reviews_test)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, df_train.sentiment, train_size = 0.75\n",
    ")\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))\n",
    "    \n",
    "final_stemmed = LogisticRegression(C=0.05)\n",
    "final_stemmed.fit(X, df_train.sentiment)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(df_test.sentiment, final_stemmed.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.876164471570832\n",
      "Accuracy for C=0.05: 0.8817860584645036\n",
      "Accuracy for C=0.25: 0.8780918727915195\n",
      "Accuracy for C=0.5: 0.8755220044972695\n",
      "Accuracy for C=1: 0.8735946032765821\n",
      "Final Accuracy: 0.8744\n"
     ]
    }
   ],
   "source": [
    "def getLemmatizedText(review_list):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in review_list]\n",
    "\n",
    "lemmatized_reviews_train = getLemmatizedText(df_train_clean.review)\n",
    "lemmatized_reviews_test = getLemmatizedText(df_test_clean.review)\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(lemmatized_reviews_train)\n",
    "X = cv.transform(lemmatized_reviews_train)\n",
    "X_test = cv.transform(lemmatized_reviews_test)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, df_train.sentiment, train_size = 0.75\n",
    ")\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))\n",
    "    \n",
    "final_lemmatized = LogisticRegression(C=0.25)\n",
    "final_lemmatized.fit(X, df_train.sentiment)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(df_test.sentiment, final_lemmatized.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.8813042081593319\n",
      "Accuracy for C=0.05: 0.8901381304208159\n",
      "Accuracy for C=0.25: 0.8920655316415034\n",
      "Accuracy for C=0.5: 0.8917442981047221\n",
      "Accuracy for C=1: 0.8919049148731127\n"
     ]
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\n",
    "ngram_vectorizer.fit(df_train_clean.review)\n",
    "X = ngram_vectorizer.transform(df_train_clean.review)\n",
    "X_test = ngram_vectorizer.transform(df_test_clean.review)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, df_train.sentiment, train_size = 0.75\n",
    ")\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.8774494057179569\n",
      "Accuracy for C=0.05: 0.8833922261484098\n",
      "Accuracy for C=0.25: 0.8790555734018631\n",
      "Accuracy for C=0.5: 0.8779312560231288\n",
      "Accuracy for C=1: 0.8726309026662383\n",
      "Final Accuracy: 0.88176\n"
     ]
    }
   ],
   "source": [
    "wc_vectorizer = CountVectorizer(binary=False)\n",
    "wc_vectorizer.fit(df_train_clean.review)\n",
    "X = wc_vectorizer.transform(df_train_clean.review)\n",
    "X_test = wc_vectorizer.transform(df_test_clean.review)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, df_train.sentiment, train_size = 0.75, \n",
    ")\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))\n",
    "    \n",
    "final_wc = LogisticRegression(C=0.05)\n",
    "final_wc.fit(X, df_train.sentiment)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(df_test.sentiment, final_wc.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.7953742370703502\n",
      "Accuracy for C=0.05: 0.8273369739800835\n",
      "Accuracy for C=0.25: 0.8652425313202698\n",
      "Accuracy for C=0.5: 0.8737552200449727\n",
      "Accuracy for C=1: 0.88082235785416\n",
      "Final Accuracy: 0.88232\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(df_train_clean.review)\n",
    "X = tfidf_vectorizer.transform(df_train_clean.review)\n",
    "X_test = tfidf_vectorizer.transform(df_test_clean.review)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, df_train.sentiment, train_size = 0.75\n",
    ")\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))\n",
    "    \n",
    "final_tfidf = LogisticRegression(C=1)\n",
    "final_tfidf.fit(X, df_train.sentiment)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(df_test.sentiment, final_tfidf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.8851590106007067\n",
      "Accuracy for C=0.05: 0.8859620944426598\n",
      "Accuracy for C=0.25: 0.8859620944426598\n",
      "Accuracy for C=0.5: 0.8858014776742692\n",
      "Accuracy for C=1: 0.8859620944426598\n"
     ]
    }
   ],
   "source": [
    "stop_words = ['in', 'of', 'at', 'a', 'the']\n",
    "ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 3), stop_words=stop_words)\n",
    "ngram_vectorizer.fit(df_train_clean.review)\n",
    "X = ngram_vectorizer.transform(df_train_clean.review)\n",
    "X_test = ngram_vectorizer.transform(df_test_clean.review)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, df_train.sentiment, train_size = 0.75\n",
    ")\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    svm = LinearSVC(C=c)\n",
    "    svm.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, svm.predict(X_val))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.90052\n"
     ]
    }
   ],
   "source": [
    "final_svm = LinearSVC(C=0.01)\n",
    "final_svm.fit(X, df_train.sentiment)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(df_test.sentiment, final_svm.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
